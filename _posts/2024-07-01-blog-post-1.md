---
title: 'Tackling the Theory Part of MIT 6.036 with No Python Coding Experience: A Personal Journey'
date: 2024-07-01
permalink: /posts/2024/07/blog-post-1/
tags:
  - cool posts
  - category1
  - category2
---

Taking MIT 6.036 (Introduction to Machine Learning) was an eye-opening and transformative experience. Coming from a non-coding background, I decided to focus primarily on the theory part of the course, and what I walked away with has given me a strong understanding of machine learning at a conceptual level. Here’s a recap of my experience, the challenges I faced, and what I learned along the way.

High-Quality Lectures: Engaging and Insightful
One of the highlights of MIT 6.036 was the top-notch lectures. Even though I wasn’t able to attend the live sessions, the recorded versions were incredibly well-done—clear, engaging, and packed with insights. The professor did a fantastic job of breaking down complex concepts into manageable pieces, which is no small feat given the subject matter.

For instance, when we covered neural networks, they didn’t just throw equations at us from the start. Instead, they carefully built up the intuition behind how neural networks work—drawing analogies, providing simple examples, and gradually introducing the math. This approach made the learning process feel approachable and manageable, even for someone like me with no prior coding experience.

Deep Dive into Machine Learning Theory
One of the reasons I focused on the theory part of MIT 6.036 was to build a solid foundation of how machine learning models work under the hood. The theory covered in the course is invaluable because it gives you a deep understanding of how different models behave under different conditions. Some of the most important concepts I gained a strong intuition for include:
Bias-variance tradeoff: Understanding the balance between model simplicity and accuracy. I now appreciate why simpler models can sometimes outperform more complex ones in real-world situations, especially when dealing with limited data.

Loss functions: One of the key takeaways was learning how to interpret loss functions and what their values mean for model performance. The idea that optimizing a model boils down to minimizing loss became a guiding principle for understanding everything from linear regression to neural networks.

Model complexity and regularization: I gained an appreciation for how regularization techniques like L1 and L2 penalties can prevent overfitting by controlling model complexity. This concept became clear not just in theory but in the context of how real-world data might behave.

The focus on theory without the immediate pressure of coding allowed me to really grasp these foundational concepts, which are critical for making sense of machine learning algorithms.

Understanding Theory Without Coding
Focusing solely on the theory part of machine learning allowed me to step back and see the big picture. I didn’t have to worry about implementing algorithms right away, so I could take my time understanding the mathematics and logic behind each model. This approach was particularly helpful in areas like:

Model Learning and Optimization: I now understand how models learn from data. Whether it’s through optimizing a loss function or calculating distances between data points like in k-nearest neighbors, the theory gave me clarity on the learning process.

Algorithm Trade-offs: Another major takeaway was developing an intuition for the trade-offs between different models. For example, I now understand why you might choose a simple model like logistic regression for certain tasks, where interpretability and speed are key, while a more complex model like a neural network might be overkill depending on the problem. This knowledge will be invaluable when I eventually move to practical coding and real-world applications.

The Path Forward
Though I focused on the theory, I’m excited about the practical side of machine learning, too. Armed with a deep understanding of the concepts, I feel ready to dive into Python and start implementing what I’ve learned in code. The next step for me is to work through the programming assignments and explore real-world data projects where I can apply everything from linear models to neural networks.

Final Thoughts
Completing the theory part of MIT 6.036 was an incredibly rewarding journey. The lectures were high-quality, and the focus on theory gave me a strong foundation in machine learning. If you’re someone who is new to coding or intimidated by the practical aspects, focusing on the theoretical concepts first is a great way to ease into the subject. You’ll gain a deeper understanding of how models work, why they behave the way they do, and how to make sense of the math behind them.

I’m looking forward to combining this theoretical knowledge with hands-on experience in the future, but for now, I’m proud of what I’ve learned—and excited for what comes next.
